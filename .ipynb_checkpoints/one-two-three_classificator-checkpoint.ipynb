{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objetivo principal: clasificar en 4 clases: fondo, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esta balanceado???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls ./gestos/train/0 | wc -l\n",
    "!ls ./gestos/train/1 | wc -l\n",
    "!ls ./gestos/train/2 | wc -l\n",
    "!ls ./gestos/train/3 | wc -l\n",
    "\n",
    "!ls ./gestos/test/0 | wc -l\n",
    "!ls ./gestos/test/1 | wc -l\n",
    "!ls ./gestos/test/2 | wc -l\n",
    "!ls ./gestos/test/3 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./gestos/valid/3 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sí, lo esta!, punto para nosotros..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handling imgs and colormaps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('gestos/train/1/D1.jpg')\n",
    "lum_img= img[:, :, 0]\n",
    "lum_img2= img[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1_axes[0, 0].plot(1, 1)\n",
    "#axs[1, 1].scatter(x, y)\n",
    "fig1, f1_axes = plt.subplots(ncols=2, nrows=2, constrained_layout=True)\n",
    "\n",
    "\n",
    "f1_axes[0, 0].set_title(\"normal\")\n",
    "f1_axes[0, 1].imshow(lum_img)\n",
    "f1_axes[1, 0].imshow(lum_img, cmap= \"hot\")\n",
    "f1_axes[1, 1].imshow(lum_img, cmap= 'nipy_spectral')\n",
    "f1_axes[0, 0].imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    torchvision.transforms.Resize(224),\n",
    "    #torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data set and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#(OneTwoThree dataset)\n",
    "OTT_train_dataset = torchvision.datasets.ImageFolder('gestos/train/', transform = transform)\n",
    "train_data_loader = torch.utils.data.DataLoader(OTT_train_dataset,\n",
    "                                          batch_size=16, ##TODO: ajustar este tamaño\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data set and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(OneTwoThree dataset)\n",
    "OTT_valid_dataset = torchvision.datasets.ImageFolder('gestos/test/', transform = transform)\n",
    "valid_data_loader = torch.utils.data.DataLoader(OTT_valid_dataset,\n",
    "                                          batch_size=16, ##TODO: ajustar este tamaño\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#bloque de comprension de donde queda la imagen \n",
    "iterable= iter(train_data_loader)\n",
    "elemento_del_iterable = next(iterable) #contiene en el [0] los 16 ejemplos de las fotos y en el [1] las clases de estos\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(foto[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CNN  SM  MLP_3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTT_Classifier(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (mpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (activation): ReLU()\n",
       "  (activation2): Softmax(dim=None)\n",
       "  (linear1): Linear(in_features=44944, out_features=120, bias=True)\n",
       "  (linear2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (linear3): Linear(in_features=84, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-5494ef081d57>:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  h = self.activation2(self.linear3(h))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2528, 0.2637, 0.2552, 0.2283]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OTT_Classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(OTT_Classifier, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(kernel_size=4, in_channels=3, out_channels=6)\n",
    "        self.conv2 = torch.nn.Conv2d(kernel_size=4, in_channels=6, out_channels=16)\n",
    "        self.mpool = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.activation2 = torch.nn.Softmax(dim=1)\n",
    "        self.linear1 = torch.nn.Linear(in_features=16*53*53 ,out_features=120)\n",
    "        self.linear2 = torch.nn.Linear(in_features=120 ,out_features=84)\n",
    "        self.linear3 = torch.nn.Linear(in_features=84 ,out_features=4)\n",
    "        \n",
    "        pass\n",
    "                     \n",
    "    def forward(self, x):\n",
    "        h = self.mpool((self.activation(self.conv1(x))))\n",
    "        h = self.mpool((self.activation(self.conv2(h))))\n",
    "        #print(h.shape)\n",
    "        #view o reshape\n",
    "        h = h.view(-1, self.linear1.in_features)\n",
    "        h = self.activation(self.linear1(h))\n",
    "        h = self.activation(self.linear2(h))\n",
    "        h = self.activation2(self.linear3(h))\n",
    "        return h\n",
    "                     \n",
    "                    \n",
    "                     \n",
    "     #hacer pasar un ejemplo \n",
    "#image = torch.tensor(np.zeros((1,3,200,200)), dtype= torch.float32) #fake example\n",
    "model = OTT_Classifier()\n",
    "display(model)\n",
    "model.forward(OTT_valid_dataset[1][0].unsqueeze(0))#.size()                \n",
    "                     \n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Loss, Accuracy\n",
    "\n",
    "model = OTT_Classifier()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "max_epochs = 2  \n",
    "\n",
    "#tensorboard --logdir=/tmp/tensorboard/\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cuda:0')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Esto es lo que hace el engine de entrenamiento\n",
    "def train_one_step(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    yhat = model.forward(x)\n",
    "    loss = criterion(yhat, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item() # Este output puede llamar luego como trainer.state.output\n",
    "\n",
    "# Esto es lo que hace el engine de evaluación\n",
    "def evaluate_one_step(engine, batch):\n",
    "    with torch.no_grad():\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        yhat = model.forward(x)\n",
    "        #loss = criterion(yhat, y)\n",
    "        return yhat, y\n",
    "\n",
    "trainer = Engine(train_one_step)\n",
    "evaluator = Engine(evaluate_one_step)\n",
    "metrics = {'Loss': Loss(criterion), 'Acc': Accuracy()}\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(evaluator, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-5494ef081d57>:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  h = self.activation2(self.linear3(h))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "\n",
    "# Contexto de escritura de datos para tensorboard\n",
    "with SummaryWriter(log_dir=f'/tmp/tensorboard/run{time.time_ns()}') as writer:\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED(every=1)) # Cada 1 epocas\n",
    "    def log_results(engine):\n",
    "        # Evaluo el conjunto de entrenamiento\n",
    "        evaluator.run(train_data_loader) \n",
    "        writer.add_scalar(\"train/loss\", evaluator.state.metrics['Loss'], engine.state.epoch)\n",
    "        writer.add_scalar(\"train/accy\", evaluator.state.metrics['Acc'], engine.state.epoch)\n",
    "        # Evaluo el conjunto de validación\n",
    "        evaluator.run(valid_data_loader) \n",
    "        writer.add_scalar(\"valid/loss\", evaluator.state.metrics['Loss'], engine.state.epoch)\n",
    "        writer.add_scalar(\"valid/accy\", evaluator.state.metrics['Acc'], engine.state.epoch)\n",
    "    # Guardo el mejor modelo en validación\n",
    "        print(\"epoch\")\n",
    "    best_model_handler = ModelCheckpoint(dirname='.', require_empty=False, filename_prefix=\"best\", n_saved=1,\n",
    "                                         score_function=lambda engine: -engine.state.metrics['Loss'],\n",
    "                                         score_name=\"val_loss\")\n",
    "\n",
    "    # Lo siguiente se ejecuta cada ves que termine el loop de validación\n",
    "    evaluator.add_event_handler(Events.COMPLETED, \n",
    "                                best_model_handler, {'ceroK': model})\n",
    "\n",
    "    trainer.run(train_data_loader, max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OTT_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo de entrenamiento base \n",
    "\n",
    "for epoch in range(num_epochs): # Durante un cierto número de épocas\n",
    "        for minibatch in data: # Para cada minibatch de datos\n",
    "            optimizer.zero_grad() # Limpiamos los gradientes\n",
    "            x, y = minibatch # Desempaquetamos\n",
    "            yhat = model.forward(x) # Predecimos\n",
    "            loss = criterion(yhat, y) # Evaluamos\n",
    "            loss.backward() # Calculamos los gradientes\n",
    "            optimizer.step() # Actualizamos los parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metricas de modelo  #test #matriz de confusion #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = OTT_Classifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.load_state_dict(torch.load('./basura/best_lenet5_val_loss=-0.0336.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.load_state_dict(torch.load('./best_ceroK_val_loss=-0.0000.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-408.5563, -116.3786, -204.5054,  -55.4185]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.forward(OTT_test_dataset[31][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OTT_test_dataset[31][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OTT_train_dataset[6000][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cybenkos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bibliografia\n",
    "\n",
    "#apis\n",
    "https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\n",
    "    \n",
    "    \n",
    "#tutoriales\n",
    "\n",
    "https://medium.com/jun-devpblog/pytorch-1-transform-imagefolder-dataloader-7f75f0a460c0\n",
    "    https://www.kaggle.com/androbomb/using-cnn-to-classify-images-w-pytorch "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
